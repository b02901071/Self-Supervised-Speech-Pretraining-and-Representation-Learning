model:                                                  # downstream model config
  tasnet: 'Transformer'  # 'ConvTasnet' or 'Transformer'
  transformer:
    input_dim: &input_dim 512                                         # `int`, 39 for mfcc, 40 for fmllr, 80 for fbank, 160 for mel
    downsample_rate: 4                                    # stacked consecutive features vectors to reduce the length of input sequences by this factor.
    hidden_size: 768                                      # Size of the encoder layers and the pooler layer.
    num_hidden_layers: 3                                  # Number of hidden layers in the Transformer encoder.
    num_attention_heads: 12                               # Number of attention heads for each attention layer in the Transformer encoder.
    intermediate_size: 3072                               # The size of the "intermediate" (i.e., feed-forward) layer in the Transformer encoder.
    hidden_act: "gelu"                                    # The non-linear activation function (function or string) in the encoder and pooler. If string, "gelu", "relu" and "swish" are supported.
    hidden_dropout_prob: 0.1                              # The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.
    attention_probs_dropout_prob: 0.1                     # The dropout ratio for the attention probabilities.
    initializer_range: 0.02                               # The sttdev of the truncated_normal_initializer for initializing all weight matrices.
    layer_norm_eps: "1e-12"                               # The epsilon used by LayerNorm.
    mask_proportion: 0.15                                 # mask this percentage of all spectrogram frames in each sequence at random during MAM training                        
    mask_consecutive_min: 0.01                               # mask this amount of consecutive frames
    mask_consecutive_max: 0.01                               # mask this amount of consecutive frames
    mask_allow_overlap: True                              # allow overlap masking
    mask_bucket_ratio: 1.2                                # only used when overlap is not allowed. sample a mask from each bucket in size of [sampled mask_consecutive * mask_bucket_ratio]
    mask_frequency: 16                                    # mask maximum this amount of frequency bands, set to 0 for no frequency mask
    noise_proportion: 0.15                                # for this percentage of the time, Gaussian noise will be applied on all frames during MAM training, set to 0 for no noise
    prune_headids: None                                   # Usage: 0,1,2,12-15 will prune headids [0,1,2,12,13,14]. headids = layerid * head_num + headid_in_layer
    share_layer: True                                     # Share layer weights
    pos_enc: 'Conv'
    downsample_type: 'Conv' # 'Conv' or 'simple'


  separation:
    n_filters: *input_dim                                      # ConvTasnet: 512, fbank: 80, mel: 160
    kernel_size: &kernel_size 16                                    # ConvTasnet: 16, spectrogram: 200 for 25ms
    stride: &stride 8                                          # ConvTasnet: 8, spectrogram: 80 for 10ms
    sample_rate: &sample_rate 8000
    segment: &seg_len 3                                       # length of segment in second
    n_src: &n_src 1
    fb_name: 'free'
    p_inv: null


optimizer: 
  learning_rate: "1e-4"                                 # Learning rate for opt. "4e-4" for 'data/libri_mel160_subword5000', "2e-4" for 'data/libri_fmllr_cmvn'
  loss_scale: 0                                         # Loss scale to improve fp16 numeric stability. Only used when apex is set to True. 0: dynamic loss scaling. positive power of 2: static loss scaling.
  warmup_proportion: 0.07                               # Proportion of training to perform linear rate warmup.
  gradient_accumulation_steps: 1                        # Number of updates steps to accumulate before performing a backward/update pass
  gradient_clipping: 1.0                                # Maximum gradient norm


dataloader:
  n_jobs: 6                                             # Subprocess used for torch Dataloader
  batch_size: 12                                         # training batch size, 12 for pre-train, 6 for cpc exp
  dev_batch_size: 12                                    # used for dev/test splits
  max_timestep: 0                                       # Max length for audio feature (0 for no restriction)

  data_path: 'data/wav8k/min'                    # Source data path, 'data/libri_fmllr_cmvn', or 'data/libri_mfcc_cmvn', or 'data/libri_mel160_subword5000' for different preprocessing features
  train_set: ['train-100']                        # ['train-clean-100', 'train-clean-360', 'train-other-500'] for pre-training. ['train-clean-360'] or ['train-clean-100'] for libri phone exp or cpc phone exp, respectively.
  dev_set: ['dev']                                #
  test_set: ['test']                              #
  train_proportion: 1.0                                 # Currently only effect the `phone classification task`, use this percent of `train_set` for downstream task training to demonstrate mockingjay generality
  task: 'enh_single'
  sample_rate: *sample_rate
  n_src: *n_src
  segment: *seg_len


runner:
  # Training options
  apex: False                                           # Use APEX (see https://github.com/NVIDIA/apex for more details)
  total_steps: 500000                                   # total steps for training, a step is a batch of update
  log_step: 1                                        # log training status every this amount of training steps
  save_step: 2000                                       # save model every this amount of training steps
  max_keep: 2                                           # maximum number of model ckpt to keep during training


